{"title":"<p style=\"color:black,text-align:center\">Logistic Regression </p>","markdown":{"yaml":{"title":"<p style=\"color:black,text-align:center\">Logistic Regression </p>","author":[{"name":"<font color=#ff6600><b>Biometrics Unit</b></font>"}],"affiliation":"<font color=#ff6600><International Institute of Tropical Agriculture (IITA)><font color=#ff6600>"},"headingText":"[ **Introduction**]{style=\"color: #234F1E;\"}","containsRefs":false,"markdown":"\n\n\nLogistic regression, also known as the logistic model or logit model, examines how multiple independent variables relate to a categorical dependent variable. It predicts the probability of an event happening by modeling data to fit a logistic curve.\n\nLogistic regression is a supervised machine learning algorithm that accomplishes binary classification tasks by predicting the probability of an outcome, event, or observation. The model delivers a binary or dichotomous outcome limited to two possible outcomes: yes/no, 0/1, or true/false.\n\nLogistic regression is a type of statistical model that is used to predict the probability of a certain event happening. It may be used to determine if an email is spam or not, as well as diagnose illnesses by determining whether certain symptoms are present or absent based on test results from patients. It works by taking input variables and transforming them into a probability value between 0 and 1, where 0 represents a low probability and 1 represents a high probability.\n\nFor example, a researcher wants to measure the poverty status of farmers in a certain locality using their annual income and decided to classify them as poor (1) and not poor (0) based on a certain threshold. Then logistic regression could be used to determine the factors influencing poverty among the farmers using predictor variables like gender, age, farm size, household size, years of farming experience, etc.\n\nThe reason it is named \"logistic\" is that an S-shaped curve (sigmoid function) is produced when the input variables are transformed using a mathematical function known as the logistic function.\n\n# [ **Advantages of Logistic Regression** ]{style=\"color: #234F1E;\"}\n\nThe logistic regression analysis has several advantages in machine learning. These are highlighted below.\n\n<img src=\"images/advanlog.png\" alt=\"Advantages of Logistic Regression\" style=\"width:50%;\"/>\n\n[Source](https://www.spiceworks.com/tech/artificial-intelligence/articles/what-is-logistic-regression/)\n\n1.  It is easier to implement machine learning models: Logistic regression is computationally efficient compared to many other machine learning methods, making it easier to implement, interpret, and train.\n\n2.  Optimal for linearly separable data: Logistic regression is specifically designed for binary classification tasks, effectively categorizing data into two distinct groups when the data is linearly separable.\n\n3.  Provides valuable insights: Logistic regression coefficients indicate both the strength and direction of the relationship between predictor variables and the outcome.\n\n# [ **The Logistic Curve** ]{style=\"color: #234F1E;\"}\n\nLogistic regression is a technique used to model the relationship between a binary (dichotomous) response variable ( y ) and a numerical predictor variable ( x ). It fits a logistic curve, which is an S-shaped or sigmoid curve, to represent how ( y ) changes as ( x ) varies. This method is particularly useful when ( y ) represents binary outcomes coded as 0 (failure) or 1 (success), such as in cases of modeling population growth or other similar scenarios.\n\nLogistic regression fits α and β, the regression coefficients. The logistic or logit function is used to transform an ‘S’-shaped curve into an approximately straight line and to change the range of the proportion from \\[0 – 1\\] to \\[(-∞) - (+∞)\\]\n\n<img src=\"images/logcurve.png\" alt=\"Logistic Curve\" style=\"width:50%;\"/>\n\n[Source](https://images.spiceworks.com/wp-content/uploads/2022/04/11040521/46-4-e1715636469361.png)\n\n<b>\n\n<p style=\"text-align: center; color: black;\">\n\nlogit(y)= ln (odds)= ln ($\\frac{P}{1 - P}$ ) = $\\alpha$ + $\\beta$X\n\n</p>\n\n</b>\n\n# [ **Assumptions**]{style=\"color: #234F1E;\"}\n\n-   **B**inary Outcome: The dependent variable should be binary in nature, i.e. it should take on one of two possible vales coded as 0 and 1, \"success\" and \"failure\", \"yes\" and \"no\".\n\n-   **I**ndependence of Errors: This implies that the error for each observation in the dataset should not be related to the error for any other observation. If violations of independence are detected, this may indicate the need to consider a different model or to account for correlation or clustering in the data using other methods such as mixed effects model.\n\n-   **L**inearity of the Logit: This means that the relationship between the independent variables and the log-odds of the outcome is linear. This means that the effect of the independent variables on the log-odds of the outcome is constant across the range of the independent variables. Logistic regression can accommodate non-linear relationships between the independent and dependent variables by employing a non-linear log transformation of the linear regression framework.\n\n-   **Large Sample Size**: A relatively large sample size is required to detect meaningful effects and to ensure stability of estimates in Logistic regression. A small sample size can lead to overfitting, where the model captures noise rather than the underlying signal in the data, and underpowered statistical tests, which may fail to detect significant effects due to inadequate sample size.\n\n-   **Outliers**: The dataset is assumed to be free of extreme outliers and significant observations for the purposes of logistic regression. To deal with outliers, you can do any of the following:\n\n    1.  Replace the outliers with a mean or median value\n    2.  Eliminate the outliers\n    3.  Keep the outliers and maintain a record while reporting the regression results.\n\n-   **No Multicollinearity**: The assumption of no or low multicollinearity among independent variables is quite vital in logistic regression. When two or more explanatory variables have a high degree of correlation with one another, the regression model cannot obtain unique or independent information from them. This phenomenon is known as multicollinearity. A high enough degree of correlation between variables may make it difficult to fit and comprehend the model.\n\nThe acronym **BILLION** gives a useful way to remember the six conditions that makes up the Logistic Regression Model.\n\n<img src=\"images/assumlog.png\" width=\"50%\"/>\n\n[Source](https://images.spiceworks.com/wp-content/uploads/2022/04/11041034/50-e1715636496628.png)\n\n# [ **Comparison Between Linear Regression and Logistic Regression** ]{style=\"color: #234F1E;\"}\n\nThe following are the comparison that exist between linear and logistic regression\n\n<img src=\"images/loglin.png\"/>\n\nLinear regression is not suitable for binary dependent variables due to several reasons. Firstly, the distribution of the binary variable ( Y ) is not normal; it follows a Bernoulli distribution rather than a Gaussian distribution assumed by linear regression. Secondly, using linear regression may lead to incomparable results because the scale and interpretation of the left-hand side (the binary outcome) and the right-hand side (predictors) of the model are fundamentally different when applied to a binary outcome.\n\nThe difference between logistic regression model and a linear regression model is that the outcome variable in logistic regression is binary and dichotomous.\n\n# [ **Models of Logistic Regression** ]{style=\"color: #234F1E;\"}\n\nThere are basically 3 types of logistic regression model, these are:\n\n1.  Binary Logistic Regression\n2.  Multinomial Logistic Regression\n3.  Ordinal Logistic Regression\n\n# [ **Binary Logistic Regression** ]{style=\"color: #234F1E;\"}\n\nBinary logistic regression belongs to the broader category of statistical models known as generalized linear models. What sets binary logistic regression apart from other models within this category is its specific application to dependent variables that have two distinct levels.\n\nAs seen above, binary logistic regression is suitable when the dependent variable has two categories, and the independent variables are either continuous or categorical. Binary logistic regression is used when we are trying to predict a dependent variable with only two outcomes (dichotomous variable), for example, yes and No.\n\nLogistic regression has the coefficients of parameters like the linear regression and in the addition has the odd ratio which is the exponential of the coefficient.\n\nOdd ratio is $\\varepsilon^{\\beta}$\n\nThe equation for binary logistic regression expression is as shown below:\n\n$log\\frac{p}{1 - p}$ = Y= $\\beta_0$ + $\\beta_1X_1$ + $\\beta_2X_2$ + ... + $\\beta_kX_k$\n\nWhere $log\\frac{p}{1 - p}$ is the odd ratio (Dependent variables)\n\n$\\beta_0$ = Constant or intercept term in the equation.\n\n$\\beta_1$,...$\\beta_k$ are the logistic regression coefficients (Coefficient of variable )\n\n$X_1$...$X_k$ = Independent variables\n\n$\\varepsilon$ = Error Term\n\n## [ Example ]{style=\"color: #002D62;\"}\n\nA sugar cane farmer wants to visually select seedlings to plant in another location. The decision is to select (1) or reject a seedling based on the number of stalks, height (m), stalk diameter, and seedling cane yield (kg). In this case, four of the explanatory variables are quantitative, one is qualitative and the response variable is dichotomous, therefore the logistic regression can the used for the analysis.\n\n```{r}\n#|include=FALSE\n#|warning =FALSE\n#|message=TRUE\n\n# load libraries\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(caret)\n\nknitr::opts_chunk$set(echo = TRUE)\n```\n\n```{r}\n#|cache=TRUE\n# read in data\ndat <- read_excel(\"Sugar cane.xlsx\", sheet = \"Sugar cane\")\nhead(dat) # print the first six rows\ndat <- dat %>% mutate(across(c(Choice,Variety), factor)) #convert \n```\n\n**The Logistic Model**\n\n```{r}\n#|warning=FALSE\n#|eval=TRUE\n\nmodel_log <- glm(Choice ~ Cane + Diameter + Height, \n             data = dat, family = binomial)\n\nsummary(model_log)\n```\n\n**Formula:** The model predicts Choice (a binary outcome) based on three predictors: Cane, Diameter, and Height.\n\n**Family:** The model uses a binomial distribution, suitable for binary response variables. **Coefficients Intercept:** The estimated intercept is -5.2139. This represents the log-odds of the outcome (Choice = 1) when all predictors are at their reference level (or zero). However, the high standard error (15.5265) indicates that this estimate is not statistically significant (p-value = 0.7370), meaning it is not reliable.\n\n**Cane:** The coefficient for Cane is 2.3901, with a p-value of 0.0351, which is statistically significant (denoted by \\*). This suggests that for each one-unit increase in Cane, the log-odds of choosing the outcome (Choice = 1) increases by approximately 2.39. In practical terms, this implies that Cane has a positive influence on the likelihood of the event happening.\n\n**Diameter:** The coefficient for Diameter is -6.8924, with a p-value of 0.1607, indicating it is not statistically significant. A negative coefficient suggests that as Diameter increases, the log-odds of the outcome decrease, but we cannot conclude that this effect is meaningful due to the p-value being above the common threshold of 0.05.\n\n**Height:** The coefficient for Height is 0.8381, but like Diameter, it has a very high p-value (0.8992), implying it is not statistically significant. Therefore, Height does not appear to have a meaningful impact on the outcome.\n\n**Model Fit Information**\n\n**Null Deviance:** 40.381, which measures the difference in the likelihood of the model with only an intercept compared to the saturated model.\n\n**Residual Deviance:** 11.797, which is considerably lower than the null deviance, suggesting that the predictors explain a significant amount of variance in the response variable. The degrees of freedom for residuals (26) suggests there are enough observations after accounting for the parameters estimated.\n\n**AIC (Akaike Information Criterion):** 19.797, a measure used for model comparison, lower values indicate a better fit when comparing multiple models.\n\nAmong the predictors, only cane seems to have a statistically significant impact on the likelihood of the choice of seedling selected or not selected. In contrast, Diameter and Height do not appear to significantly affect the choice of seedling selected or not selected. The model does fit the data better than a null model, as indicated by the reduction in deviance, but the overall significance and impact of some predictors are weak.\n\n### Variable Importance\n\n```{r}\n#|eval=TRUE\n\ncaret::varImp(model_log)\nexp(coef(model_log))\n```\n\n```{r}\n#|warning=FALSE\n\ncbind(\"Odds ratio\" = round(exp(coef(model_log)),4), \n      \"P-value\" = round(coef(summary(model_log)),4)[,4], \n      round(exp(confint.default(model_log, level = 0.95)),4))\n```\n\nCane has an odd ratio of 10.9150. This implies that for each unit increase in cane, the odds of the outcome are approximately 10.92 times higher, with a P-value of 0.0351 indicates it is statistically significant (evidence to suggest Cane has a meaningful effect on the outcome).\n\nDiameter has an odd ratio of 0.0010. This indicate that for each unit increase in diameter, the odds of the choice of seedling made decrease significantly (close to zero).With a P-value of 0.1607 shows it is not statistically significant.\n\nHeight has an odd ratio of 2.3119, implying that for each unit increase in height, the odds of the choice of seedling made are increased by about 2.31 times. With a p-value(0.8992). It shows it is not statistically significant.\n\nHence, we can say that cane is a significant predictor of the choice of seedlings.\n\n# [ **Multinomial Logistic Regression** ]{style=\"color: #234F1E;\"}\n\nMultinomial logistic regression (often just called \"multinomial regression\") is used to predict a nominal dependent variable given one or more independent variables. Multinomial logistic regression is used when the dependent variable has more than two categories.\n\nThe equation for multinomial logistic regression is as shown below:\n\nLet K be the number of possible outcomes.\n\nLet P(Y=k\\|X) be the probability of outcome k given predictors X.\n\nLet $\\beta_{kj}$ be the coefficient for predictor j in predicting outcome k\n\nThen, the probability of outcome k is given by:\n\n$$P(Y=k|X) = \\frac{\\exp(\\beta_{ko} + beta_{k1*X1} + ... + (\\beta_{kp*Xp})}{\\sum(exp(β_{j0} + β_{j1*X1} + ... + β_{jp*Xp}))}$$\n\n$β_{ko}$ is the intercept for outcome k.\n\nThe summation in the denominator is over all possible outcomes j.\n\n**Interpretation**\n\nThe formula calculates the probability of each possible outcome given the values of the predictor variables.\n\nThe coefficients (β values) represent the impact of the predictors on the log-odds of each outcome compared to a reference category.\n\nY= $\\beta_0$ + $\\beta_1X_1$ + $\\beta_2X_2$ + ... + $\\beta_kX_k$\n\nWhere Y = Dependent variables (Odd ratio)\n\n$\\beta_0$ = Constant\n\n$\\beta_1$ = Coefficient of variable\n\n$X_1$ = Independent variables\n\nE = Error Term\n\n## [ Example ]{style=\"color: #002D62;\"}\n\nTo predict the species of new flowers using a multinomial logistic regression model fitted on the iris dataset.\n\nThe VGAM (Vector Generalized Linear and Additive Models) package in R Programming Language provides a suite of functions for fitting a variety of regression models.\n\n```{r}\n#|message=FALSE\n#|warning = FALSE\n\nlibrary(VGAM) \n  \n# Load and prepare the data \ndata(iris) \n  \n# Convert the species variable to a factor \niris$Species <- as.factor(iris$Species) \n  \n\n```\n\nThis loads the library and the data, then converts the variable 'species' from character to a factor.\n\n```{r}\n#|warning=FALSE\n#|message=FALSE\n#|cache = FALSE\n\n\n\n\n# Fit a multinomial logistic regression model \nfit <- vglm(Species ~ Sepal.Length  \n            + Sepal.Width \n            + Petal.Length \n            + Petal.Width, \n            data = iris, \n            family = multinomial) \n  \n# Print the model summary \nsummary(fit) \n\n```\n\n```{r}\nlibrary(nnet) \ndata(iris) \n  \n# Fit multinomial logistic regression model \nmodel <- multinom(Species ~ Petal.Length \n                  + Petal.Width \n                  + Sepal.Length \n                  + Sepal.Width, \n                  data = iris) \n```\n\n```{r}\n# Predict flower species for new data \nnew_flo <- data.frame(Petal.Length = 1.5, \n                       Petal.Width = 0.3,  \n                       Sepal.Length = 4.5,  \n                       Sepal.Width = 3.1) \npredict(model, newdata = new_flo, type = \"class\") \n```\n\nWe replace the values in new_flo with the actual measurements of the new flower you want to predict. This gives the predicted species of the new flower (e.g., \"setosa\", \"versicolor\", or \"virginica\").\n\n# [ **Ordinal Logistic Regression** ]{style=\"color: #234F1E;\"}\n\nOrdinal logistic regression or (ordinal regression) is used to model the relationship between **an ordinal response variable** and **one or more explanatory variables**. It is used to predict an **ordinal dependent variable** given **one or more independent variables**. The explanatory variables may be either continuous or categorical. We explore how one or more independent variables relate to the probability of an ordinal outcome being categorized into a specific or higher category using ordinal logistic regression.\n\nOrdinal Logistic Regression has assumptions which varies from the general logistic regression. They are listed below:\n\n**Assumptions**\n\n• The dependent variable is measured on an ordinal level.\n\n• One or more of the independent variables are either continuous, categorical or ordinal.\n\n• No multi-collinearity - i.e. when two or more independent variables are highly correlated with each other.\n\n• Proportional Odds - i.e. that each independent variable has an identical effect at each cumulative split of the ordinal dependent variable.\n\n**Ordinal Regression Model:**\n\n$$logit(P(Y \\geq j)) = \\beta_{jo} + \\beta_1x_1 + ...+ \\beta_px_p$$ Where:\n\n$P(Y \\geq j)$ = the cummulative probability of the response variable within category j\n\n$\\beta_{jo}$ = the threshold parameter for category j\n\n$\\beta_{1}$, $\\beta_2$, ..., $\\beta_{jp}$ are model coefficient associated with the predictor variables $X_1$, $X_2$, ..., $X_p$\n\n## [ Example ]{style=\"color: #002D62;\"}\n\n-   Let's use a hypothetical dataset where we predict customer satisfaction (ordinal variable: Low, Medium, High) based on factors like age, income, and product usage.\n\n```{r}\n\n# Load necessary libraries\nlibrary(tidyverse)\nlibrary(ordinal)\n\n# Simulated data (replace with your actual data)\nset.seed(123)\ndata <- data.frame(\n  satisfaction = factor(sample(c(\"Low\", \"Medium\", \"High\"), 100, replace = TRUE), ordered = TRUE),\n  age = sample(25:65, 100, replace = TRUE),\n  income = sample(30000:100000, 100, replace = TRUE),\n  usage = sample(1:5, 100, replace = TRUE)\n)\n\n\nhead(data)\n```\n\n```{r}\nsum(is.na(data)) #checking for missing values\nstr(data)\n\n```\n\n```{r}\n\nplot(data) # to check for proportional odds\n```\n\n```{r}\n# Fit the ordinal logistic regression model\n\nlibrary(ordinal)\nmodel <- clm(satisfaction ~ age + usage , data = data)\nmodel\n\n# Summary of the model\nsummary(model)\n```\n","srcMarkdownNoYaml":"\n\n# [ **Introduction**]{style=\"color: #234F1E;\"}\n\nLogistic regression, also known as the logistic model or logit model, examines how multiple independent variables relate to a categorical dependent variable. It predicts the probability of an event happening by modeling data to fit a logistic curve.\n\nLogistic regression is a supervised machine learning algorithm that accomplishes binary classification tasks by predicting the probability of an outcome, event, or observation. The model delivers a binary or dichotomous outcome limited to two possible outcomes: yes/no, 0/1, or true/false.\n\nLogistic regression is a type of statistical model that is used to predict the probability of a certain event happening. It may be used to determine if an email is spam or not, as well as diagnose illnesses by determining whether certain symptoms are present or absent based on test results from patients. It works by taking input variables and transforming them into a probability value between 0 and 1, where 0 represents a low probability and 1 represents a high probability.\n\nFor example, a researcher wants to measure the poverty status of farmers in a certain locality using their annual income and decided to classify them as poor (1) and not poor (0) based on a certain threshold. Then logistic regression could be used to determine the factors influencing poverty among the farmers using predictor variables like gender, age, farm size, household size, years of farming experience, etc.\n\nThe reason it is named \"logistic\" is that an S-shaped curve (sigmoid function) is produced when the input variables are transformed using a mathematical function known as the logistic function.\n\n# [ **Advantages of Logistic Regression** ]{style=\"color: #234F1E;\"}\n\nThe logistic regression analysis has several advantages in machine learning. These are highlighted below.\n\n<img src=\"images/advanlog.png\" alt=\"Advantages of Logistic Regression\" style=\"width:50%;\"/>\n\n[Source](https://www.spiceworks.com/tech/artificial-intelligence/articles/what-is-logistic-regression/)\n\n1.  It is easier to implement machine learning models: Logistic regression is computationally efficient compared to many other machine learning methods, making it easier to implement, interpret, and train.\n\n2.  Optimal for linearly separable data: Logistic regression is specifically designed for binary classification tasks, effectively categorizing data into two distinct groups when the data is linearly separable.\n\n3.  Provides valuable insights: Logistic regression coefficients indicate both the strength and direction of the relationship between predictor variables and the outcome.\n\n# [ **The Logistic Curve** ]{style=\"color: #234F1E;\"}\n\nLogistic regression is a technique used to model the relationship between a binary (dichotomous) response variable ( y ) and a numerical predictor variable ( x ). It fits a logistic curve, which is an S-shaped or sigmoid curve, to represent how ( y ) changes as ( x ) varies. This method is particularly useful when ( y ) represents binary outcomes coded as 0 (failure) or 1 (success), such as in cases of modeling population growth or other similar scenarios.\n\nLogistic regression fits α and β, the regression coefficients. The logistic or logit function is used to transform an ‘S’-shaped curve into an approximately straight line and to change the range of the proportion from \\[0 – 1\\] to \\[(-∞) - (+∞)\\]\n\n<img src=\"images/logcurve.png\" alt=\"Logistic Curve\" style=\"width:50%;\"/>\n\n[Source](https://images.spiceworks.com/wp-content/uploads/2022/04/11040521/46-4-e1715636469361.png)\n\n<b>\n\n<p style=\"text-align: center; color: black;\">\n\nlogit(y)= ln (odds)= ln ($\\frac{P}{1 - P}$ ) = $\\alpha$ + $\\beta$X\n\n</p>\n\n</b>\n\n# [ **Assumptions**]{style=\"color: #234F1E;\"}\n\n-   **B**inary Outcome: The dependent variable should be binary in nature, i.e. it should take on one of two possible vales coded as 0 and 1, \"success\" and \"failure\", \"yes\" and \"no\".\n\n-   **I**ndependence of Errors: This implies that the error for each observation in the dataset should not be related to the error for any other observation. If violations of independence are detected, this may indicate the need to consider a different model or to account for correlation or clustering in the data using other methods such as mixed effects model.\n\n-   **L**inearity of the Logit: This means that the relationship between the independent variables and the log-odds of the outcome is linear. This means that the effect of the independent variables on the log-odds of the outcome is constant across the range of the independent variables. Logistic regression can accommodate non-linear relationships between the independent and dependent variables by employing a non-linear log transformation of the linear regression framework.\n\n-   **Large Sample Size**: A relatively large sample size is required to detect meaningful effects and to ensure stability of estimates in Logistic regression. A small sample size can lead to overfitting, where the model captures noise rather than the underlying signal in the data, and underpowered statistical tests, which may fail to detect significant effects due to inadequate sample size.\n\n-   **Outliers**: The dataset is assumed to be free of extreme outliers and significant observations for the purposes of logistic regression. To deal with outliers, you can do any of the following:\n\n    1.  Replace the outliers with a mean or median value\n    2.  Eliminate the outliers\n    3.  Keep the outliers and maintain a record while reporting the regression results.\n\n-   **No Multicollinearity**: The assumption of no or low multicollinearity among independent variables is quite vital in logistic regression. When two or more explanatory variables have a high degree of correlation with one another, the regression model cannot obtain unique or independent information from them. This phenomenon is known as multicollinearity. A high enough degree of correlation between variables may make it difficult to fit and comprehend the model.\n\nThe acronym **BILLION** gives a useful way to remember the six conditions that makes up the Logistic Regression Model.\n\n<img src=\"images/assumlog.png\" width=\"50%\"/>\n\n[Source](https://images.spiceworks.com/wp-content/uploads/2022/04/11041034/50-e1715636496628.png)\n\n# [ **Comparison Between Linear Regression and Logistic Regression** ]{style=\"color: #234F1E;\"}\n\nThe following are the comparison that exist between linear and logistic regression\n\n<img src=\"images/loglin.png\"/>\n\nLinear regression is not suitable for binary dependent variables due to several reasons. Firstly, the distribution of the binary variable ( Y ) is not normal; it follows a Bernoulli distribution rather than a Gaussian distribution assumed by linear regression. Secondly, using linear regression may lead to incomparable results because the scale and interpretation of the left-hand side (the binary outcome) and the right-hand side (predictors) of the model are fundamentally different when applied to a binary outcome.\n\nThe difference between logistic regression model and a linear regression model is that the outcome variable in logistic regression is binary and dichotomous.\n\n# [ **Models of Logistic Regression** ]{style=\"color: #234F1E;\"}\n\nThere are basically 3 types of logistic regression model, these are:\n\n1.  Binary Logistic Regression\n2.  Multinomial Logistic Regression\n3.  Ordinal Logistic Regression\n\n# [ **Binary Logistic Regression** ]{style=\"color: #234F1E;\"}\n\nBinary logistic regression belongs to the broader category of statistical models known as generalized linear models. What sets binary logistic regression apart from other models within this category is its specific application to dependent variables that have two distinct levels.\n\nAs seen above, binary logistic regression is suitable when the dependent variable has two categories, and the independent variables are either continuous or categorical. Binary logistic regression is used when we are trying to predict a dependent variable with only two outcomes (dichotomous variable), for example, yes and No.\n\nLogistic regression has the coefficients of parameters like the linear regression and in the addition has the odd ratio which is the exponential of the coefficient.\n\nOdd ratio is $\\varepsilon^{\\beta}$\n\nThe equation for binary logistic regression expression is as shown below:\n\n$log\\frac{p}{1 - p}$ = Y= $\\beta_0$ + $\\beta_1X_1$ + $\\beta_2X_2$ + ... + $\\beta_kX_k$\n\nWhere $log\\frac{p}{1 - p}$ is the odd ratio (Dependent variables)\n\n$\\beta_0$ = Constant or intercept term in the equation.\n\n$\\beta_1$,...$\\beta_k$ are the logistic regression coefficients (Coefficient of variable )\n\n$X_1$...$X_k$ = Independent variables\n\n$\\varepsilon$ = Error Term\n\n## [ Example ]{style=\"color: #002D62;\"}\n\nA sugar cane farmer wants to visually select seedlings to plant in another location. The decision is to select (1) or reject a seedling based on the number of stalks, height (m), stalk diameter, and seedling cane yield (kg). In this case, four of the explanatory variables are quantitative, one is qualitative and the response variable is dichotomous, therefore the logistic regression can the used for the analysis.\n\n```{r}\n#|include=FALSE\n#|warning =FALSE\n#|message=TRUE\n\n# load libraries\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(caret)\n\nknitr::opts_chunk$set(echo = TRUE)\n```\n\n```{r}\n#|cache=TRUE\n# read in data\ndat <- read_excel(\"Sugar cane.xlsx\", sheet = \"Sugar cane\")\nhead(dat) # print the first six rows\ndat <- dat %>% mutate(across(c(Choice,Variety), factor)) #convert \n```\n\n**The Logistic Model**\n\n```{r}\n#|warning=FALSE\n#|eval=TRUE\n\nmodel_log <- glm(Choice ~ Cane + Diameter + Height, \n             data = dat, family = binomial)\n\nsummary(model_log)\n```\n\n**Formula:** The model predicts Choice (a binary outcome) based on three predictors: Cane, Diameter, and Height.\n\n**Family:** The model uses a binomial distribution, suitable for binary response variables. **Coefficients Intercept:** The estimated intercept is -5.2139. This represents the log-odds of the outcome (Choice = 1) when all predictors are at their reference level (or zero). However, the high standard error (15.5265) indicates that this estimate is not statistically significant (p-value = 0.7370), meaning it is not reliable.\n\n**Cane:** The coefficient for Cane is 2.3901, with a p-value of 0.0351, which is statistically significant (denoted by \\*). This suggests that for each one-unit increase in Cane, the log-odds of choosing the outcome (Choice = 1) increases by approximately 2.39. In practical terms, this implies that Cane has a positive influence on the likelihood of the event happening.\n\n**Diameter:** The coefficient for Diameter is -6.8924, with a p-value of 0.1607, indicating it is not statistically significant. A negative coefficient suggests that as Diameter increases, the log-odds of the outcome decrease, but we cannot conclude that this effect is meaningful due to the p-value being above the common threshold of 0.05.\n\n**Height:** The coefficient for Height is 0.8381, but like Diameter, it has a very high p-value (0.8992), implying it is not statistically significant. Therefore, Height does not appear to have a meaningful impact on the outcome.\n\n**Model Fit Information**\n\n**Null Deviance:** 40.381, which measures the difference in the likelihood of the model with only an intercept compared to the saturated model.\n\n**Residual Deviance:** 11.797, which is considerably lower than the null deviance, suggesting that the predictors explain a significant amount of variance in the response variable. The degrees of freedom for residuals (26) suggests there are enough observations after accounting for the parameters estimated.\n\n**AIC (Akaike Information Criterion):** 19.797, a measure used for model comparison, lower values indicate a better fit when comparing multiple models.\n\nAmong the predictors, only cane seems to have a statistically significant impact on the likelihood of the choice of seedling selected or not selected. In contrast, Diameter and Height do not appear to significantly affect the choice of seedling selected or not selected. The model does fit the data better than a null model, as indicated by the reduction in deviance, but the overall significance and impact of some predictors are weak.\n\n### Variable Importance\n\n```{r}\n#|eval=TRUE\n\ncaret::varImp(model_log)\nexp(coef(model_log))\n```\n\n```{r}\n#|warning=FALSE\n\ncbind(\"Odds ratio\" = round(exp(coef(model_log)),4), \n      \"P-value\" = round(coef(summary(model_log)),4)[,4], \n      round(exp(confint.default(model_log, level = 0.95)),4))\n```\n\nCane has an odd ratio of 10.9150. This implies that for each unit increase in cane, the odds of the outcome are approximately 10.92 times higher, with a P-value of 0.0351 indicates it is statistically significant (evidence to suggest Cane has a meaningful effect on the outcome).\n\nDiameter has an odd ratio of 0.0010. This indicate that for each unit increase in diameter, the odds of the choice of seedling made decrease significantly (close to zero).With a P-value of 0.1607 shows it is not statistically significant.\n\nHeight has an odd ratio of 2.3119, implying that for each unit increase in height, the odds of the choice of seedling made are increased by about 2.31 times. With a p-value(0.8992). It shows it is not statistically significant.\n\nHence, we can say that cane is a significant predictor of the choice of seedlings.\n\n# [ **Multinomial Logistic Regression** ]{style=\"color: #234F1E;\"}\n\nMultinomial logistic regression (often just called \"multinomial regression\") is used to predict a nominal dependent variable given one or more independent variables. Multinomial logistic regression is used when the dependent variable has more than two categories.\n\nThe equation for multinomial logistic regression is as shown below:\n\nLet K be the number of possible outcomes.\n\nLet P(Y=k\\|X) be the probability of outcome k given predictors X.\n\nLet $\\beta_{kj}$ be the coefficient for predictor j in predicting outcome k\n\nThen, the probability of outcome k is given by:\n\n$$P(Y=k|X) = \\frac{\\exp(\\beta_{ko} + beta_{k1*X1} + ... + (\\beta_{kp*Xp})}{\\sum(exp(β_{j0} + β_{j1*X1} + ... + β_{jp*Xp}))}$$\n\n$β_{ko}$ is the intercept for outcome k.\n\nThe summation in the denominator is over all possible outcomes j.\n\n**Interpretation**\n\nThe formula calculates the probability of each possible outcome given the values of the predictor variables.\n\nThe coefficients (β values) represent the impact of the predictors on the log-odds of each outcome compared to a reference category.\n\nY= $\\beta_0$ + $\\beta_1X_1$ + $\\beta_2X_2$ + ... + $\\beta_kX_k$\n\nWhere Y = Dependent variables (Odd ratio)\n\n$\\beta_0$ = Constant\n\n$\\beta_1$ = Coefficient of variable\n\n$X_1$ = Independent variables\n\nE = Error Term\n\n## [ Example ]{style=\"color: #002D62;\"}\n\nTo predict the species of new flowers using a multinomial logistic regression model fitted on the iris dataset.\n\nThe VGAM (Vector Generalized Linear and Additive Models) package in R Programming Language provides a suite of functions for fitting a variety of regression models.\n\n```{r}\n#|message=FALSE\n#|warning = FALSE\n\nlibrary(VGAM) \n  \n# Load and prepare the data \ndata(iris) \n  \n# Convert the species variable to a factor \niris$Species <- as.factor(iris$Species) \n  \n\n```\n\nThis loads the library and the data, then converts the variable 'species' from character to a factor.\n\n```{r}\n#|warning=FALSE\n#|message=FALSE\n#|cache = FALSE\n\n\n\n\n# Fit a multinomial logistic regression model \nfit <- vglm(Species ~ Sepal.Length  \n            + Sepal.Width \n            + Petal.Length \n            + Petal.Width, \n            data = iris, \n            family = multinomial) \n  \n# Print the model summary \nsummary(fit) \n\n```\n\n```{r}\nlibrary(nnet) \ndata(iris) \n  \n# Fit multinomial logistic regression model \nmodel <- multinom(Species ~ Petal.Length \n                  + Petal.Width \n                  + Sepal.Length \n                  + Sepal.Width, \n                  data = iris) \n```\n\n```{r}\n# Predict flower species for new data \nnew_flo <- data.frame(Petal.Length = 1.5, \n                       Petal.Width = 0.3,  \n                       Sepal.Length = 4.5,  \n                       Sepal.Width = 3.1) \npredict(model, newdata = new_flo, type = \"class\") \n```\n\nWe replace the values in new_flo with the actual measurements of the new flower you want to predict. This gives the predicted species of the new flower (e.g., \"setosa\", \"versicolor\", or \"virginica\").\n\n# [ **Ordinal Logistic Regression** ]{style=\"color: #234F1E;\"}\n\nOrdinal logistic regression or (ordinal regression) is used to model the relationship between **an ordinal response variable** and **one or more explanatory variables**. It is used to predict an **ordinal dependent variable** given **one or more independent variables**. The explanatory variables may be either continuous or categorical. We explore how one or more independent variables relate to the probability of an ordinal outcome being categorized into a specific or higher category using ordinal logistic regression.\n\nOrdinal Logistic Regression has assumptions which varies from the general logistic regression. They are listed below:\n\n**Assumptions**\n\n• The dependent variable is measured on an ordinal level.\n\n• One or more of the independent variables are either continuous, categorical or ordinal.\n\n• No multi-collinearity - i.e. when two or more independent variables are highly correlated with each other.\n\n• Proportional Odds - i.e. that each independent variable has an identical effect at each cumulative split of the ordinal dependent variable.\n\n**Ordinal Regression Model:**\n\n$$logit(P(Y \\geq j)) = \\beta_{jo} + \\beta_1x_1 + ...+ \\beta_px_p$$ Where:\n\n$P(Y \\geq j)$ = the cummulative probability of the response variable within category j\n\n$\\beta_{jo}$ = the threshold parameter for category j\n\n$\\beta_{1}$, $\\beta_2$, ..., $\\beta_{jp}$ are model coefficient associated with the predictor variables $X_1$, $X_2$, ..., $X_p$\n\n## [ Example ]{style=\"color: #002D62;\"}\n\n-   Let's use a hypothetical dataset where we predict customer satisfaction (ordinal variable: Low, Medium, High) based on factors like age, income, and product usage.\n\n```{r}\n\n# Load necessary libraries\nlibrary(tidyverse)\nlibrary(ordinal)\n\n# Simulated data (replace with your actual data)\nset.seed(123)\ndata <- data.frame(\n  satisfaction = factor(sample(c(\"Low\", \"Medium\", \"High\"), 100, replace = TRUE), ordered = TRUE),\n  age = sample(25:65, 100, replace = TRUE),\n  income = sample(30000:100000, 100, replace = TRUE),\n  usage = sample(1:5, 100, replace = TRUE)\n)\n\n\nhead(data)\n```\n\n```{r}\nsum(is.na(data)) #checking for missing values\nstr(data)\n\n```\n\n```{r}\n\nplot(data) # to check for proportional odds\n```\n\n```{r}\n# Fit the ordinal logistic regression model\n\nlibrary(ordinal)\nmodel <- clm(satisfaction ~ age + usage , data = data)\nmodel\n\n# Summary of the model\nsummary(model)\n```\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.555","editor":"visual","theme":"cosmo","title":"<p style=\"color:black,text-align:center\">Logistic Regression </p>","author":[{"name":"<font color=#ff6600><b>Biometrics Unit</b></font>"}],"affiliation":"<font color=#ff6600><International Institute of Tropical Agriculture (IITA)><font color=#ff6600>"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}