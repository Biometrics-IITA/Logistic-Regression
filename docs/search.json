[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\nLogistic Regression\n",
    "section": "",
    "text": "Logistic regression, also known as the logistic model or logit model, examines how multiple independent variables relate to a categorical dependent variable. It predicts the probability of an event happening by modeling data to fit a logistic curve.\nLogistic regression is a supervised machine learning algorithm that accomplishes binary classification tasks by predicting the probability of an outcome, event, or observation. The model delivers a binary or dichotomous outcome limited to two possible outcomes: yes/no, 0/1, or true/false.\nLogistic regression is a type of statistical model that is used to predict the probability of a certain event happening. It may be used to determine if an email is spam or not, as well as diagnose illnesses by determining whether certain symptoms are present or absent based on test results from patients. It works by taking input variables and transforming them into a probability value between 0 and 1, where 0 represents a low probability and 1 represents a high probability.\nFor example, a researcher wants to measure the poverty status of farmers in a certain locality using their annual income and decided to classify them as poor (1) and not poor (0) based on a certain threshold. Then logistic regression could be used to determine the factors influencing poverty among the farmers using predictor variables like gender, age, farm size, household size, years of farming experience, etc.\nThe reason it is named “logistic” is that an S-shaped curve (sigmoid function) is produced when the input variables are transformed using a mathematical function known as the logistic function."
  },
  {
    "objectID": "index.html#example",
    "href": "index.html#example",
    "title": "\nLogistic Regression\n",
    "section": "Example",
    "text": "Example\nA sugar cane farmer wants to visually select seedlings to plant in another location. The decision is to select (1) or reject a seedling based on the number of stalks, height (m), stalk diameter, and seedling cane yield (kg). In this case, four of the explanatory variables are quantitative, one is qualitative and the response variable is dichotomous, therefore the logistic regression can the used for the analysis.\n\n#|include=FALSE\n#|warning =FALSE\n#|message=TRUE\n\n# load libraries\nlibrary(readxl)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(caret)\n\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\nknitr::opts_chunk$set(echo = TRUE)\n\n\n#|cache=TRUE\n# read in data\ndat &lt;- read_excel(\"Sugar cane.xlsx\", sheet = \"Sugar cane\")\nhead(dat) # print the first six rows\n\n# A tibble: 6 × 6\n  Choice Stalks Height Diameter  Cane Variety\n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1      1     23   2.37     1.7  12.4        2\n2      0     11   2.25     1.68  5.49       3\n3      0      9   2.5      1.93  6.59       2\n4      1     25   2.4      2.12 21.2        2\n5      1     20   2.5      1.7  11.4        1\n6      0     12   1.9      1.51  4.08       1\n\ndat &lt;- dat %&gt;% mutate(across(c(Choice,Variety), factor)) #convert \n\nThe Logistic Model\n\n#|warning=FALSE\n#|eval=TRUE\n\nmodel_log &lt;- glm(Choice ~ Cane + Diameter + Height, \n             data = dat, family = binomial)\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nsummary(model_log)\n\n\nCall:\nglm(formula = Choice ~ Cane + Diameter + Height, family = binomial, \n    data = dat)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)  -5.2139    15.5265  -0.336   0.7370  \nCane          2.3901     1.1340   2.108   0.0351 *\nDiameter     -6.8924     4.9139  -1.403   0.1607  \nHeight        0.8381     6.6179   0.127   0.8992  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 40.381  on 29  degrees of freedom\nResidual deviance: 11.797  on 26  degrees of freedom\nAIC: 19.797\n\nNumber of Fisher Scoring iterations: 8\n\n\nFormula: The model predicts Choice (a binary outcome) based on three predictors: Cane, Diameter, and Height.\nFamily: The model uses a binomial distribution, suitable for binary response variables. Coefficients Intercept: The estimated intercept is -5.2139. This represents the log-odds of the outcome (Choice = 1) when all predictors are at their reference level (or zero). However, the high standard error (15.5265) indicates that this estimate is not statistically significant (p-value = 0.7370), meaning it is not reliable.\nCane: The coefficient for Cane is 2.3901, with a p-value of 0.0351, which is statistically significant (denoted by *). This suggests that for each one-unit increase in Cane, the log-odds of choosing the outcome (Choice = 1) increases by approximately 2.39. In practical terms, this implies that Cane has a positive influence on the likelihood of the event happening.\nDiameter: The coefficient for Diameter is -6.8924, with a p-value of 0.1607, indicating it is not statistically significant. A negative coefficient suggests that as Diameter increases, the log-odds of the outcome decrease, but we cannot conclude that this effect is meaningful due to the p-value being above the common threshold of 0.05.\nHeight: The coefficient for Height is 0.8381, but like Diameter, it has a very high p-value (0.8992), implying it is not statistically significant. Therefore, Height does not appear to have a meaningful impact on the outcome.\nModel Fit Information\nNull Deviance: 40.381, which measures the difference in the likelihood of the model with only an intercept compared to the saturated model.\nResidual Deviance: 11.797, which is considerably lower than the null deviance, suggesting that the predictors explain a significant amount of variance in the response variable. The degrees of freedom for residuals (26) suggests there are enough observations after accounting for the parameters estimated.\nAIC (Akaike Information Criterion): 19.797, a measure used for model comparison, lower values indicate a better fit when comparing multiple models.\nAmong the predictors, only cane seems to have a statistically significant impact on the likelihood of the choice of seedling selected or not selected. In contrast, Diameter and Height do not appear to significantly affect the choice of seedling selected or not selected. The model does fit the data better than a null model, as indicated by the reduction in deviance, but the overall significance and impact of some predictors are weak.\n\nVariable Importance\n\n#|eval=TRUE\n\ncaret::varImp(model_log)\n\n           Overall\nCane     2.1076503\nDiameter 1.4026300\nHeight   0.1266381\n\nexp(coef(model_log))\n\n (Intercept)         Cane     Diameter       Height \n 0.005440294 10.914966068  0.001015469  2.311918797 \n\n\n\n#|warning=FALSE\n\ncbind(\"Odds ratio\" = round(exp(coef(model_log)),4), \n      \"P-value\" = round(coef(summary(model_log)),4)[,4], \n      round(exp(confint.default(model_log, level = 0.95)),4))\n\n            Odds ratio P-value  2.5 %       97.5 %\n(Intercept)     0.0054  0.7370 0.0000 8.949811e+10\nCane           10.9150  0.0351 1.1823 1.007648e+02\nDiameter        0.0010  0.1607 0.0000 1.546670e+01\nHeight          2.3119  0.8992 0.0000 9.934237e+05\n\n\nCane has an odd ratio of 10.9150. This implies that for each unit increase in cane, the odds of the outcome are approximately 10.92 times higher, with a P-value of 0.0351 indicates it is statistically significant (evidence to suggest Cane has a meaningful effect on the outcome).\nDiameter has an odd ratio of 0.0010. This indicate that for each unit increase in diameter, the odds of the choice of seedling made decrease significantly (close to zero).With a P-value of 0.1607 shows it is not statistically significant.\nHeight has an odd ratio of 2.3119, implying that for each unit increase in height, the odds of the choice of seedling made are increased by about 2.31 times. With a p-value(0.8992). It shows it is not statistically significant.\nHence, we can say that cane is a significant predictor of the choice of seedlings."
  },
  {
    "objectID": "index.html#example-1",
    "href": "index.html#example-1",
    "title": "\nLogistic Regression\n",
    "section": "Example",
    "text": "Example\nTo predict the species of new flowers using a multinomial logistic regression model fitted on the iris dataset.\nThe VGAM (Vector Generalized Linear and Additive Models) package in R Programming Language provides a suite of functions for fitting a variety of regression models.\n\n#|message=FALSE\n#|warning = FALSE\n\nlibrary(VGAM) \n\nLoading required package: stats4\n\n\nLoading required package: splines\n\n\n\nAttaching package: 'VGAM'\n\n\nThe following object is masked from 'package:caret':\n\n    predictors\n\n# Load and prepare the data \ndata(iris) \n  \n# Convert the species variable to a factor \niris$Species &lt;- as.factor(iris$Species) \n\nThis loads the library and the data, then converts the variable ‘species’ from character to a factor.\n\n#|warning=FALSE\n#|message=FALSE\n#|cache = FALSE\n\n\n\n\n# Fit a multinomial logistic regression model \nfit &lt;- vglm(Species ~ Sepal.Length  \n            + Sepal.Width \n            + Petal.Length \n            + Petal.Width, \n            data = iris, \n            family = multinomial) \n\nWarning in checkwz(wz, M = M, trace = trace, wzepsilon = control$wzepsilon): 2\ndiagonal elements of the working weights variable 'wz' have been replaced by\n1.819e-12\n\n\nWarning in checkwz(wz, M = M, trace = trace, wzepsilon = control$wzepsilon): 13\ndiagonal elements of the working weights variable 'wz' have been replaced by\n1.819e-12\n\n\nWarning in checkwz(wz, M = M, trace = trace, wzepsilon = control$wzepsilon): 22\ndiagonal elements of the working weights variable 'wz' have been replaced by\n1.819e-12\n\n\nWarning in checkwz(wz, M = M, trace = trace, wzepsilon = control$wzepsilon): 34\ndiagonal elements of the working weights variable 'wz' have been replaced by\n1.819e-12\n\n\nWarning in checkwz(wz, M = M, trace = trace, wzepsilon = control$wzepsilon): 39\ndiagonal elements of the working weights variable 'wz' have been replaced by\n1.819e-12\n\n\nWarning in checkwz(wz, M = M, trace = trace, wzepsilon = control$wzepsilon): 41\ndiagonal elements of the working weights variable 'wz' have been replaced by\n1.819e-12\n\n\nWarning in checkwz(wz, M = M, trace = trace, wzepsilon = control$wzepsilon): 47\ndiagonal elements of the working weights variable 'wz' have been replaced by\n1.819e-12\n\n\nWarning in checkwz(wz, M = M, trace = trace, wzepsilon = control$wzepsilon): 50\ndiagonal elements of the working weights variable 'wz' have been replaced by\n1.819e-12\n\n\nWarning in checkwz(wz, M = M, trace = trace, wzepsilon = control$wzepsilon): 54\ndiagonal elements of the working weights variable 'wz' have been replaced by\n1.819e-12\n\n\nWarning in checkwz(wz, M = M, trace = trace, wzepsilon = control$wzepsilon): 59\ndiagonal elements of the working weights variable 'wz' have been replaced by\n1.819e-12\n\n\nWarning in checkwz(wz, M = M, trace = trace, wzepsilon = control$wzepsilon): 63\ndiagonal elements of the working weights variable 'wz' have been replaced by\n1.819e-12\n\n\nWarning in checkwz(wz, M = M, trace = trace, wzepsilon = control$wzepsilon): 78\ndiagonal elements of the working weights variable 'wz' have been replaced by\n1.819e-12\n\n\nWarning in checkwz(wz, M = M, trace = trace, wzepsilon = control$wzepsilon): 91\ndiagonal elements of the working weights variable 'wz' have been replaced by\n1.819e-12\n\n\nWarning in slot(family, \"linkinv\")(eta, extra = extra): fitted probabilities\nnumerically 0 or 1 occurred\n\n\nWarning in tfun(mu = mu, y = y, w = w, res = FALSE, eta = eta, extra = extra):\nfitted values close to 0 or 1\n\n\nWarning in checkwz(wz, M = M, trace = trace, wzepsilon = control$wzepsilon): 96\ndiagonal elements of the working weights variable 'wz' have been replaced by\n1.819e-12\n\n\nWarning in slot(family, \"linkinv\")(eta, extra = extra): fitted probabilities\nnumerically 0 or 1 occurred\n\n\nWarning in tfun(mu = mu, y = y, w = w, res = FALSE, eta = eta, extra = extra):\nfitted values close to 0 or 1\n\n\nWarning in slot(family, \"linkinv\")(eta, extra = extra): fitted probabilities\nnumerically 0 or 1 occurred\n\n\nWarning in tfun(mu = mu, y = y, w = w, res = FALSE, eta = eta, extra = extra):\nfitted values close to 0 or 1\n\n\nWarning in checkwz(wz, M = M, trace = trace, wzepsilon = control$wzepsilon): 97\ndiagonal elements of the working weights variable 'wz' have been replaced by\n1.819e-12\n\n\nWarning in slot(family, \"linkinv\")(eta, extra = extra): fitted probabilities\nnumerically 0 or 1 occurred\n\n\nWarning in tfun(mu = mu, y = y, w = w, res = FALSE, eta = eta, extra = extra):\nfitted values close to 0 or 1\n\n\nWarning in slot(family, \"linkinv\")(eta, extra = extra): fitted probabilities\nnumerically 0 or 1 occurred\n\n\nWarning in tfun(mu = mu, y = y, w = w, res = FALSE, eta = eta, extra = extra):\nfitted values close to 0 or 1\n\n\nWarning in slot(family, \"linkinv\")(eta, extra = extra): fitted probabilities\nnumerically 0 or 1 occurred\n\n\nWarning in tfun(mu = mu, y = y, w = w, res = FALSE, eta = eta, extra = extra):\nfitted values close to 0 or 1\n\n\nWarning in slot(family, \"linkinv\")(eta, extra = extra): fitted probabilities\nnumerically 0 or 1 occurred\n\n\nWarning in tfun(mu = mu, y = y, w = w, res = FALSE, eta = eta, extra = extra):\nfitted values close to 0 or 1\n\n\nWarning in slot(family, \"linkinv\")(eta, extra = extra): fitted probabilities\nnumerically 0 or 1 occurred\n\n\nWarning in tfun(mu = mu, y = y, w = w, res = FALSE, eta = eta, extra = extra):\nfitted values close to 0 or 1\n\n\nWarning in slot(family, \"linkinv\")(eta, extra = extra): fitted probabilities\nnumerically 0 or 1 occurred\n\n\nWarning in tfun(mu = mu, y = y, w = w, res = FALSE, eta = eta, extra = extra):\nfitted values close to 0 or 1\n\n\nWarning in slot(family, \"linkinv\")(eta, extra = extra): fitted probabilities\nnumerically 0 or 1 occurred\n\n\nWarning in tfun(mu = mu, y = y, w = w, res = FALSE, eta = eta, extra = extra):\nfitted values close to 0 or 1\n\n\nWarning in slot(family, \"linkinv\")(eta, extra = extra): fitted probabilities\nnumerically 0 or 1 occurred\n\n\nWarning in tfun(mu = mu, y = y, w = w, res = FALSE, eta = eta, extra = extra):\nfitted values close to 0 or 1\n\n\nWarning in vglm.fitter(x = x, y = y, w = w, offset = offset, Xm2 = Xm2, : some\nquantities such as z, residuals, SEs may be inaccurate due to convergence at a\nhalf-step\n\n# Print the model summary \nsummary(fit) \n\nWarning in temp1@family@linkinv(eta = temp1@predictors, extra = temp1@extra):\nfitted probabilities numerically 0 or 1 occurred\n\n\nWarning in temp1@family@linkinv(eta = temp1@predictors, extra = temp1@extra):\nfitted probabilities numerically 0 or 1 occurred\nWarning in temp1@family@linkinv(eta = temp1@predictors, extra = temp1@extra):\nfitted probabilities numerically 0 or 1 occurred\nWarning in temp1@family@linkinv(eta = temp1@predictors, extra = temp1@extra):\nfitted probabilities numerically 0 or 1 occurred\nWarning in temp1@family@linkinv(eta = temp1@predictors, extra = temp1@extra):\nfitted probabilities numerically 0 or 1 occurred\nWarning in temp1@family@linkinv(eta = temp1@predictors, extra = temp1@extra):\nfitted probabilities numerically 0 or 1 occurred\nWarning in temp1@family@linkinv(eta = temp1@predictors, extra = temp1@extra):\nfitted probabilities numerically 0 or 1 occurred\nWarning in temp1@family@linkinv(eta = temp1@predictors, extra = temp1@extra):\nfitted probabilities numerically 0 or 1 occurred\nWarning in temp1@family@linkinv(eta = temp1@predictors, extra = temp1@extra):\nfitted probabilities numerically 0 or 1 occurred\nWarning in temp1@family@linkinv(eta = temp1@predictors, extra = temp1@extra):\nfitted probabilities numerically 0 or 1 occurred\n\n\n\nCall:\nvglm(formula = Species ~ Sepal.Length + Sepal.Width + Petal.Length + \n    Petal.Width, family = multinomial, data = iris)\n\nCoefficients: \n                Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept):1     35.490  22666.953      NA       NA  \n(Intercept):2     42.638     25.708   1.659   0.0972 .\nSepal.Length:1     9.495   6729.217      NA       NA  \nSepal.Length:2     2.465      2.394   1.030   0.3032  \nSepal.Width:1     12.300   3143.611      NA       NA  \nSepal.Width:2      6.681      4.480   1.491   0.1359  \nPetal.Length:1   -22.975   4799.227  -0.005   0.9962  \nPetal.Length:2    -9.429      4.737      NA       NA  \nPetal.Width:1    -33.843   7583.502      NA       NA  \nPetal.Width:2    -18.286      9.743      NA       NA  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nNames of linear predictors: log(mu[,1]/mu[,3]), log(mu[,2]/mu[,3])\n\nResidual deviance: 11.8985 on 290 degrees of freedom\n\nLog-likelihood: -5.9493 on 290 degrees of freedom\n\nNumber of Fisher scoring iterations: 21 \n\nWarning: Hauck-Donner effect detected in the following estimate(s):\n'(Intercept):1', 'Sepal.Length:1', 'Sepal.Width:1', 'Petal.Length:2', 'Petal.Width:1', 'Petal.Width:2'\n\n\nReference group is level  3  of the response\n\n\n\nlibrary(nnet) \ndata(iris) \n  \n# Fit multinomial logistic regression model \nmodel &lt;- multinom(Species ~ Petal.Length \n                  + Petal.Width \n                  + Sepal.Length \n                  + Sepal.Width, \n                  data = iris) \n\n# weights:  18 (10 variable)\ninitial  value 164.791843 \niter  10 value 16.177348\niter  20 value 7.111438\niter  30 value 6.182999\niter  40 value 5.984028\niter  50 value 5.961278\niter  60 value 5.954900\niter  70 value 5.951851\niter  80 value 5.950343\niter  90 value 5.949904\niter 100 value 5.949867\nfinal  value 5.949867 \nstopped after 100 iterations\n\n\n\n# Predict flower species for new data \nnew_flo &lt;- data.frame(Petal.Length = 1.5, \n                       Petal.Width = 0.3,  \n                       Sepal.Length = 4.5,  \n                       Sepal.Width = 3.1) \npredict(model, newdata = new_flo, type = \"class\") \n\n[1] setosa\nLevels: setosa versicolor virginica\n\n\nWe replace the values in new_flo with the actual measurements of the new flower you want to predict. This gives the predicted species of the new flower (e.g., “setosa”, “versicolor”, or “virginica”)."
  },
  {
    "objectID": "index.html#example-2",
    "href": "index.html#example-2",
    "title": "\nLogistic Regression\n",
    "section": "Example",
    "text": "Example\n\nLet’s use a hypothetical dataset where we predict customer satisfaction (ordinal variable: Low, Medium, High) based on factors like age, income, and product usage.\n\n\n# Load necessary libraries\nlibrary(tidyverse)\nlibrary(ordinal)\n\n\nAttaching package: 'ordinal'\n\n\nThe following objects are masked from 'package:VGAM':\n\n    dgumbel, dlgamma, pgumbel, plgamma, qgumbel, rgumbel, wine\n\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\n# Simulated data (replace with your actual data)\nset.seed(123)\ndata &lt;- data.frame(\n  satisfaction = factor(sample(c(\"Low\", \"Medium\", \"High\"), 100, replace = TRUE), ordered = TRUE),\n  age = sample(25:65, 100, replace = TRUE),\n  income = sample(30000:100000, 100, replace = TRUE),\n  usage = sample(1:5, 100, replace = TRUE)\n)\n\n\nhead(data)\n\n  satisfaction age income usage\n1         High  47  73927     2\n2         High  39  36600     4\n3         High  45  87700     5\n4       Medium  61  83517     2\n5         High  32  82352     3\n6       Medium  34  96153     3\n\n\n\nsum(is.na(data)) #checking for missing values\n\n[1] 0\n\nstr(data)\n\n'data.frame':   100 obs. of  4 variables:\n $ satisfaction: Ord.factor w/ 3 levels \"High\"&lt;\"Low\"&lt;\"Medium\": 1 1 1 3 1 3 3 3 1 2 ...\n $ age         : int  47 39 45 61 32 34 58 34 46 36 ...\n $ income      : int  73927 36600 87700 83517 82352 96153 77801 61516 96074 62262 ...\n $ usage       : int  2 4 5 2 3 3 1 3 2 3 ...\n\n\n\nplot(data) # to check for proportional odds\n\n\n\n\n\n\n\n\n\n# Fit the ordinal logistic regression model\n\nlibrary(ordinal)\nmodel &lt;- clm(satisfaction ~ age + usage , data = data)\nmodel\n\nformula: satisfaction ~ age + usage\ndata:    data\n\n link  threshold nobs logLik  AIC    niter max.grad cond.H \n logit flexible  100  -108.48 224.95 3(0)  5.13e-08 9.5e+04\n\nCoefficients:\n      age     usage \n-0.005358  0.206033 \n\nThreshold coefficients:\n  High|Low Low|Medium \n   -0.2749     1.1276 \n\n# Summary of the model\nsummary(model)\n\nformula: satisfaction ~ age + usage\ndata:    data\n\n link  threshold nobs logLik  AIC    niter max.grad cond.H \n logit flexible  100  -108.48 224.95 3(0)  5.13e-08 9.5e+04\n\nCoefficients:\n       Estimate Std. Error z value Pr(&gt;|z|)\nage   -0.005358   0.017546  -0.305    0.760\nusage  0.206033   0.129595   1.590    0.112\n\nThreshold coefficients:\n           Estimate Std. Error z value\nHigh|Low    -0.2749     0.8794  -0.313\nLow|Medium   1.1276     0.8869   1.271"
  }
]